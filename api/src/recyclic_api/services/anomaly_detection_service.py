"""
Service de d√©tection d'anomalies pour le syst√®me Recyclic.

Ce service analyse les donn√©es m√©tier pour d√©tecter les anomalies
telles que les √©carts de caisse r√©p√©t√©s, les √©checs de synchronisation,
et les erreurs de classification IA.
"""

import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta, timezone
from sqlalchemy.orm import Session
from sqlalchemy import and_, or_, func, desc, text
import sqlalchemy.orm

from recyclic_api.models.cash_session import CashSession, CashSessionStatus
from recyclic_api.models.sync_log import SyncLog
from recyclic_api.models.deposit import Deposit
from recyclic_api.models.sale import Sale
from recyclic_api.models.user import User
from recyclic_api.models.login_history import LoginHistory

from recyclic_api.services.telegram_service import telegram_service

logger = logging.getLogger(__name__)


class AnomalyDetectionService:
    """
    Service de d√©tection d'anomalies m√©tier.

    Ce service analyse p√©riodiquement les donn√©es du syst√®me pour d√©tecter:
    - √âcarts de caisse r√©p√©t√©s
    - √âchecs de synchronisation fr√©quents
    - Erreurs de classification IA
    - Anomalies d'authentification
    """

    def __init__(self, db: Session):
        self.db = db
        self.anomaly_thresholds = {
            'cash_variance_threshold': 10.0,  # √âcart de caisse en ‚Ç¨
            'sync_failure_threshold': 3,      # Nombre d'√©checs cons√©cutifs
            'classification_error_threshold': 5,  # % d'erreurs de classification
            'auth_failure_threshold': 5,      # Nombre d'√©checs d'auth cons√©cutifs
        }
        # Cache simple pour √©viter les requ√™tes r√©p√©t√©es
        self._cache = {}
        self._cache_ttl = 300  # 5 minutes

    def _is_cache_valid(self, key: str) -> bool:
        """V√©rifie si le cache est encore valide pour une cl√© donn√©e."""
        if key not in self._cache:
            return False
        
        cache_time, _ = self._cache[key]
        return (datetime.now(timezone.utc) - cache_time).total_seconds() < self._cache_ttl

    def _get_from_cache(self, key: str) -> Optional[Any]:
        """R√©cup√®re une valeur du cache si elle est valide."""
        if self._is_cache_valid(key):
            return self._cache[key][1]
        return None

    def _set_cache(self, key: str, value: Any):
        """Met une valeur en cache."""
        self._cache[key] = (datetime.now(timezone.utc), value)

    async def run_anomaly_detection(self) -> Dict[str, Any]:
        """
        Ex√©cute la d√©tection d'anomalies compl√®te.

        Returns:
            Dict contenant les anomalies d√©tect√©es et les recommandations
        """
        logger.info("D√©marrage de la d√©tection d'anomalies")

        # V√©rifier le cache d'abord
        cache_key = "anomaly_detection"
        cached_result = self._get_from_cache(cache_key)
        if cached_result:
            logger.info("Utilisation du cache pour la d√©tection d'anomalies")
            return cached_result

        anomalies = {
            'cash_anomalies': await self._detect_cash_anomalies(),
            'sync_anomalies': await self._detect_sync_anomalies(),
            'auth_anomalies': await self._detect_auth_anomalies(),
            'classification_anomalies': await self._detect_classification_anomalies(),
            'timestamp': datetime.now(timezone.utc)
        }

        recommendations = self._generate_recommendations(anomalies)

        result = {
            'anomalies': anomalies,
            'recommendations': recommendations,
            'summary': {
                'total_anomalies': sum(len(v) for v in anomalies.values() if isinstance(v, list)),
                'critical_anomalies': len([a for a in anomalies.values() if isinstance(a, list) and a])
            },
            'timestamp': datetime.now(timezone.utc).isoformat()
        }

        # Mettre en cache le r√©sultat
        self._set_cache(cache_key, result)

        logger.info(f"D√©tection d'anomalies termin√©e. Anomalies d√©tect√©es: {result['summary']['total_anomalies']}")
        return result

    async def _detect_cash_anomalies(self) -> List[Dict[str, Any]]:
        """
        D√©tecte les anomalies dans les sessions de caisse.

        Returns:
            Liste des anomalies d√©tect√©es avec d√©tails
        """
        anomalies = []

        try:
            # R√©cup√©rer les sessions de caisse avec variance significative
            cutoff_date = datetime.now(timezone.utc) - timedelta(days=7)

            problematic_sessions = self.db.query(CashSession).filter(
                and_(
                    CashSession.closed_at >= cutoff_date,
                    CashSession.status == CashSessionStatus.CLOSED,
                    CashSession.variance.isnot(None),
                    or_(
                        CashSession.variance > self.anomaly_thresholds['cash_variance_threshold'],
                        CashSession.variance < -self.anomaly_thresholds['cash_variance_threshold']
                    )
                )
            ).options(
                # Optimisation: charger les relations en une seule requ√™te
                sqlalchemy.orm.joinedload(CashSession.operator),
                sqlalchemy.orm.joinedload(CashSession.site)
            ).limit(100).all()  # Limiter √† 100 r√©sultats pour √©viter les surcharges

            for session in problematic_sessions:
                anomaly = {
                    'type': 'cash_variance',
                    'severity': 'high' if abs(session.variance) > 50 else 'medium',
                    'description': f"√âcart de caisse d√©tect√©: {session.variance:.2f}‚Ç¨",
                    'details': {
                        'session_id': str(session.id),
                        'operator_id': str(session.operator_id)[:8] + '...' if session.operator_id else 'N/A',
                        'operator_name': session.operator.username[:3] + '***' if session.operator and session.operator.username else 'Inconnu',
                        'site': session.site.name if session.site else 'Inconnu',
                        'variance': float(session.variance),
                        'closing_amount': '***.**' if session.closing_amount else 'N/A',
                        'actual_amount': '***.**' if session.actual_amount else 'N/A',
                        'closed_at': session.closed_at.isoformat() if session.closed_at else None
                    }
                }
                anomalies.append(anomaly)

        except Exception as e:
            logger.error(f"Erreur lors de la d√©tection d'anomalies de caisse: {e}")
            anomalies.append({
                'type': 'detection_error',
                'severity': 'critical',
                'description': f"Erreur dans la d√©tection d'anomalies de caisse: {str(e)}",
                'details': {}
            })

        return anomalies

    async def _detect_sync_anomalies(self) -> List[Dict[str, Any]]:
        """
        D√©tecte les anomalies de synchronisation.

        Returns:
            Liste des anomalies d√©tect√©es
        """
        anomalies = []

        try:
            # Analyser les logs de synchronisation des derni√®res 24h
            cutoff_date = datetime.now(timezone.utc) - timedelta(hours=24)

            recent_syncs = self.db.query(SyncLog).filter(
                SyncLog.created_at >= cutoff_date
            ).order_by(desc(SyncLog.created_at)).limit(1000).all()  # Limiter les r√©sultats

            # Compter les √©checs par type de sync
            failure_counts = {}
            for sync in recent_syncs:
                if sync.status == 'error':
                    key = sync.sync_type
                    failure_counts[key] = failure_counts.get(key, 0) + 1

            # Identifier les syncs avec trop d'√©checs
            for sync_type, count in failure_counts.items():
                if count >= self.anomaly_thresholds['sync_failure_threshold']:
                    anomaly = {
                        'type': 'sync_failure',
                        'severity': 'high' if count >= 5 else 'medium',
                        'description': f"Trop d'√©checs de synchronisation {sync_type}: {count}",
                        'details': {
                            'sync_type': sync_type,
                            'failure_count': count,
                            'recent_syncs': len([s for s in recent_syncs if s.sync_type == sync_type]),
                            'time_range': '24h'
                        }
                    }
                    anomalies.append(anomaly)

        except Exception as e:
            logger.error(f"Erreur lors de la d√©tection d'anomalies de sync: {e}")
            anomalies.append({
                'type': 'detection_error',
                'severity': 'critical',
                'description': f"Erreur dans la d√©tection d'anomalies de sync: {str(e)}",
                'details': {}
            })

        return anomalies

    async def _detect_auth_anomalies(self) -> List[Dict[str, Any]]:
        """
        D√©tecte les anomalies d'authentification.

        Returns:
            Liste des anomalies d√©tect√©es
        """
        anomalies = []

        try:
            # Analyser les logs d'authentification des derni√®res 24h
            cutoff_date = datetime.now(timezone.utc) - timedelta(hours=24)

            recent_logins = self.db.query(LoginHistory).filter(
                LoginHistory.created_at >= cutoff_date
            ).limit(1000).all()  # Limiter les r√©sultats

            # Compter les √©checs par utilisateur
            user_failures = {}
            for login in recent_logins:
                if not login.success:
                    user_id = str(login.user_id) if login.user_id else 'unknown'
                    user_failures[user_id] = user_failures.get(user_id, 0) + 1

            # Identifier les utilisateurs avec trop d'√©checs
            for user_id, count in user_failures.items():
                if count >= self.anomaly_thresholds['auth_failure_threshold']:
                    anomaly = {
                        'type': 'auth_failure',
                        'severity': 'high' if count >= 10 else 'medium',
                        'description': f"Trop d'√©checs d'authentification pour l'utilisateur {user_id[:8]}...: {count}",
                        'details': {
                            'user_id': user_id[:8] + '...' if user_id != 'unknown' else 'unknown',
                            'failure_count': count,
                            'total_attempts': len([l for l in recent_logins if str(l.user_id) == user_id]),
                            'time_range': '24h'
                        }
                    }
                    anomalies.append(anomaly)

        except Exception as e:
            logger.error(f"Erreur lors de la d√©tection d'anomalies d'auth: {e}")
            anomalies.append({
                'type': 'detection_error',
                'severity': 'critical',
                'description': f"Erreur dans la d√©tection d'anomalies d'auth: {str(e)}",
                'details': {}
            })

        return anomalies

    async def _detect_classification_anomalies(self) -> List[Dict[str, Any]]:
        """
        D√©tecte les anomalies de classification IA.

        Note: Cette fonctionnalit√© n√©cessiterait des m√©triques de classification
        qui ne sont pas encore impl√©ment√©es dans le syst√®me actuel.
        """
        anomalies = []

        # TODO: Impl√©menter la collecte de m√©triques de classification
        # Pour l'instant, on retourne une liste vide avec une note
        logger.info("D√©tection d'anomalies de classification - fonctionnalit√© √† impl√©menter")

        return anomalies

    def _generate_recommendations(self, anomalies: Dict[str, List]) -> List[Dict[str, Any]]:
        """
        G√©n√®re des recommandations bas√©es sur les anomalies d√©tect√©es et l'analyse pr√©ventive.

        Args:
            anomalies: Dictionnaire des anomalies d√©tect√©es

        Returns:
            Liste des recommandations
        """
        recommendations = []

        # Compter les anomalies par type
        anomaly_counts = {}
        for anomaly_type, anomaly_list in anomalies.items():
            if isinstance(anomaly_list, list):
                anomaly_counts[anomaly_type] = len(anomaly_list)

        # G√©n√©rer des recommandations bas√©es sur les types d'anomalies
        if anomaly_counts.get('cash_anomalies', 0) > 0:
            recommendations.append({
                'type': 'cash_control',
                'priority': 'high',
                'title': 'Am√©liorer les contr√¥les de caisse',
                'description': 'Consid√©rer l\'impl√©mentation de doubles contr√¥les ou de v√©rifications automatis√©es pour les sessions de caisse.',
                'actions': [
                    'Former les op√©rateurs sur les bonnes pratiques de comptage',
                    'Impl√©menter un syst√®me de double v√©rification pour les montants √©lev√©s',
                    'Ajouter des alertes en temps r√©el pour les √©carts d√©tect√©s'
                ]
            })

        if anomaly_counts.get('sync_anomalies', 0) > 0:
            recommendations.append({
                'type': 'sync_monitoring',
                'priority': 'medium',
                'title': 'Surveiller les synchronisations',
                'description': 'Les √©checs de synchronisation r√©p√©t√©s indiquent un probl√®me potentiel avec le syst√®me de synchronisation.',
                'actions': [
                    'V√©rifier la connectivit√© r√©seau',
                    'Contr√¥ler les logs de synchronisation d√©taill√©s',
                    'Tester manuellement les processus de synchronisation'
                ]
            })

        if anomaly_counts.get('auth_anomalies', 0) > 0:
            recommendations.append({
                'type': 'auth_security',
                'priority': 'high' if anomaly_counts.get('auth_anomalies', 0) >= 3 else 'medium',
                'title': 'S√©curiser l\'authentification',
                'description': 'Les √©checs d\'authentification r√©p√©t√©s peuvent indiquer une tentative d\'intrusion.',
                'actions': [
                    'Bloquer temporairement les comptes avec trop d\'√©checs',
                    'Ajouter un CAPTCHA pour les tentatives r√©p√©t√©es',
                    'Surveiller les logs d\'authentification en temps r√©el'
                ]
            })

        # Recommandations de maintenance pr√©ventive (ind√©pendantes des anomalies)
        recommendations.extend(self._generate_preventive_maintenance_recommendations())

        return recommendations

    def _generate_preventive_maintenance_recommendations(self) -> List[Dict[str, Any]]:
        """
        G√©n√®re des recommandations de maintenance pr√©ventive bas√©es sur l'analyse des donn√©es.

        Returns:
            Liste des recommandations de maintenance pr√©ventive
        """
        recommendations = []

        try:
            # Analyser les donn√©es pour d√©tecter les tendances n√©cessitant de la maintenance
            cutoff_date = datetime.now(timezone.utc) - timedelta(days=30)

            # Analyser les sessions de caisse pour d√©tecter les probl√®mes r√©currents
            cash_sessions = self.db.query(CashSession).filter(
                CashSession.opened_at >= cutoff_date
            ).all()

            if len(cash_sessions) > 0:
                # Calculer le taux d'√©carts de caisse
                total_sessions = len(cash_sessions)
                sessions_with_variance = len([s for s in cash_sessions if s.variance and abs(s.variance) > 1.0])

                if total_sessions > 0:
                    variance_rate = sessions_with_variance / total_sessions

                    if variance_rate > 0.1:  # Plus de 10% des sessions ont des √©carts
                        recommendations.append({
                            'type': 'preventive_cash_training',
                            'priority': 'high',
                            'title': 'Formation sur les contr√¥les de caisse',
                            'description': f'{variance_rate*100:.1f}% des sessions de caisse ont des √©carts. Une formation pourrait am√©liorer la pr√©cision.',
                            'actions': [
                                'Organiser une session de formation sur les bonnes pratiques de comptage',
                                'Cr√©er une checklist de fermeture de caisse',
                                'Impl√©menter un syst√®me de v√©rification crois√©e'
                            ]
                        })

            # Analyser les logs de synchronisation pour d√©tecter les probl√®mes r√©currents
            sync_logs = self.db.query(SyncLog).filter(
                SyncLog.created_at >= cutoff_date
            ).all()

            if len(sync_logs) > 0:
                error_rate = len([s for s in sync_logs if s.status == 'error']) / len(sync_logs)

                if error_rate > 0.05:  # Plus de 5% d'erreurs de sync
                    recommendations.append({
                        'type': 'preventive_sync_optimization',
                        'priority': 'medium',
                        'title': 'Optimiser les synchronisations',
                        'description': f'{error_rate*100:.1f}% des synchronisations √©chouent. Une optimisation pourrait am√©liorer la fiabilit√©.',
                        'actions': [
                            'Analyser les patterns d\'√©chec dans les logs',
                            'Optimiser les param√®tres de synchronisation',
                            'Impl√©menter des retries plus intelligents'
                        ]
                    })

            # Recommandations de maintenance g√©n√©rale
            recommendations.append({
                'type': 'preventive_database_maintenance',
                'priority': 'low',
                'title': 'Maintenance pr√©ventive de la base de donn√©es',
                'description': 'Maintenance r√©guli√®re recommand√©e pour optimiser les performances.',
                'actions': [
                    'V√©rifier les index et les optimiser si n√©cessaire',
                    'Archiver les donn√©es anciennes (>1 an)',
                    'V√©rifier l\'espace disque disponible',
                    'Contr√¥ler les performances des requ√™tes lentes'
                ]
            })

            recommendations.append({
                'type': 'preventive_security_review',
                'priority': 'low',
                'title': 'R√©vision de s√©curit√© p√©riodique',
                'description': 'Audit de s√©curit√© recommand√© pour maintenir la conformit√©.',
                'actions': [
                    'V√©rifier les logs d\'authentification pour les anomalies',
                    'Contr√¥ler les permissions utilisateur',
                    'Mettre √† jour les d√©pendances de s√©curit√©',
                    'Tester les sauvegardes et restaurations'
                ]
            })

        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration des recommandations pr√©ventives: {e}")
            recommendations.append({
                'type': 'preventive_error',
                'priority': 'medium',
                'title': 'Erreur dans l\'analyse pr√©ventive',
                'description': f'Une erreur est survenue lors de l\'analyse: {str(e)}',
                'actions': [
                    'V√©rifier les logs du syst√®me de monitoring',
                    'Tester manuellement les requ√™tes de base de donn√©es',
                    'Contacter l\'administrateur syst√®me'
                ]
            })

        return recommendations

    async def send_anomaly_notifications(self, anomalies: Dict[str, Any]) -> bool:
        """
        Envoie des notifications Telegram pour les anomalies d√©tect√©es.

        Args:
            anomalies: R√©sultats de la d√©tection d'anomalies

        Returns:
            True si les notifications ont √©t√© envoy√©es avec succ√®s
        """
        try:
            critical_anomalies = anomalies.get('summary', {}).get('critical_anomalies', 0)
            total_anomalies = anomalies.get('summary', {}).get('total_anomalies', 0)

            if critical_anomalies == 0 and total_anomalies == 0:
                logger.info("Aucune anomalie d√©tect√©e, pas de notification √† envoyer")
                return True

            # Construire le message de notification
            message = "[ALERTE SYST√àME] Anomalies d√©tect√©es\n\n"

            if critical_anomalies > 0:
                message += f"üö® {critical_anomalies} anomalies CRITIQUES d√©tect√©es\n"

            if total_anomalies > 0:
                message += f"‚ö†Ô∏è {total_anomalies} anomalies au total\n"

            message += f"\nüìä R√©sum√© par type:\n"

            for anomaly_type, anomaly_list in anomalies.get('anomalies', {}).items():
                if isinstance(anomaly_list, list) and anomaly_list:
                    message += f"‚Ä¢ {anomaly_type}: {len(anomaly_list)} anomalies\n"

            message += "\nüîß Recommandations disponibles dans le syst√®me de monitoring"

            # Envoyer la notification via Telegram
            success = await telegram_service.notify_sync_failure(
                file_path="system-monitoring",
                remote_path="anomaly-detection",
                error_message=message
            )

            if success:
                logger.info("Notification d'anomalies envoy√©e avec succ√®s")
            else:
                logger.error("√âchec de l'envoi de la notification d'anomalies")

            return success

        except Exception as e:
            logger.error(f"Erreur lors de l'envoi des notifications d'anomalies: {e}")
            return False


# Factory function pour cr√©er le service
def get_anomaly_detection_service(db: Session) -> AnomalyDetectionService:
    """Factory function pour cr√©er une instance du service."""
    return AnomalyDetectionService(db)
